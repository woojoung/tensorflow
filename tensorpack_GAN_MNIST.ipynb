{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorpack GAN with MNIST data\n",
    "#### programming through these following steps\n",
    "* **Data Loading** (Dataflow)\n",
    "* **Build a model**\n",
    "* **Training & test** (CPU, GPU, or MultiGPU)\n",
    "* **Trasfer learning** (Loading a saved Model and training it agiain)\n",
    "* **Evaluation** (Loading a saved Model and test it)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# File: GAN.py\n",
    "# Author: Yuxin Wu\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorpack import (TowerTrainer,\n",
    "                        ModelDescBase, DataFlow, StagingInput)\n",
    "from tensorpack.tfutils.tower import TowerContext, TowerFuncWrapper\n",
    "from tensorpack.graph_builder import DataParallelBuilder, LeastLoadedDeviceSetter\n",
    "from tensorpack.tfutils.summary import add_moving_summary\n",
    "from tensorpack.utils.argtools import memoized\n",
    "\n",
    "####\n",
    "# ModelDescBase class에서 상속받은 내장 함수, build_graph(), inputs(), get_inputs_desc() 를 GANModelDesc class에서 사용할 수 있다. \n",
    "####\n",
    "\n",
    "class GANModelDesc(ModelDescBase):\n",
    "    def collect_variables(self, g_scope='gen', d_scope='discrim'):\n",
    "        \"\"\"\n",
    "        Assign `self.g_vars` to the parameters under scope `g_scope`,\n",
    "        and same with `self.d_vars`.\n",
    "        \"\"\"\n",
    "        self.g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, g_scope)\n",
    "        assert self.g_vars\n",
    "        self.d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, d_scope)\n",
    "        assert self.d_vars\n",
    "#         보통 GAN에서 gen, discrim scope의 vars는 optimizer를 정의하고 train step을 정의할때 var_list option의 parameter로 사용된다. \n",
    "#         그래서 tf.get_collection()에서도 TRAINABLE_VARIABLES라고 표기 되어있음.\n",
    "    \n",
    "\n",
    "        \n",
    "    def build_losses(self, logits_real, logits_fake):\n",
    "#         \"\"\"\n",
    "#         Build standard GAN loss and set `self.g_loss` and `self.d_loss`.\n",
    "#         D and G play two-player minimax game with value function V(G,D)\n",
    "#           min_G max _D V(D, G) = IE_{x ~ p_data} [log D(x)] + IE_{z ~ p_fake} [log (1 - D(G(z)))]\n",
    "#         Args:\n",
    "#             logits_real (tf.Tensor): discrim logits from real samples\n",
    "#             logits_fake (tf.Tensor): discrim logits from fake samples produced by generator\n",
    "#         \"\"\"\n",
    "        with tf.name_scope(\"GAN_loss\"):\n",
    "            score_real = tf.sigmoid(logits_real)\n",
    "            score_fake = tf.sigmoid(logits_fake)\n",
    "            tf.summary.histogram('score-real', score_real)\n",
    "            tf.summary.histogram('score-fake', score_fake)\n",
    "\n",
    "            with tf.name_scope(\"discrim\"):\n",
    "                d_loss_pos = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=logits_real, labels=tf.ones_like(logits_real)), name='loss_real')\n",
    "                d_loss_neg = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=logits_fake, labels=tf.zeros_like(logits_fake)), name='loss_fake')\n",
    "\n",
    "                d_pos_acc = tf.reduce_mean(tf.cast(score_real > 0.5, tf.float32), name='accuracy_real')\n",
    "                d_neg_acc = tf.reduce_mean(tf.cast(score_fake < 0.5, tf.float32), name='accuracy_fake')\n",
    "\n",
    "                d_accuracy = tf.add(.5 * d_pos_acc, .5 * d_neg_acc, name='accuracy')\n",
    "                self.d_loss = tf.add(.5 * d_loss_pos, .5 * d_loss_neg, name='loss')\n",
    "\n",
    "            with tf.name_scope(\"gen\"):\n",
    "                self.g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=logits_fake, labels=tf.ones_like(logits_fake)), name='loss')\n",
    "                g_accuracy = tf.reduce_mean(tf.cast(score_fake > 0.5, tf.float32), name='accuracy')\n",
    "\n",
    "            add_moving_summary(self.g_loss, self.d_loss, d_accuracy, g_accuracy)\n",
    "\n",
    "    def build_graph(self, *inputs):\n",
    "        \"\"\"\n",
    "        Have to build one tower and set the following attributes:\n",
    "        g_loss, d_loss, g_vars, d_vars.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @memoized\n",
    "    def get_optimizer(self):\n",
    "        return self.optimizer()\n",
    "\n",
    "\n",
    "class GANTrainer(TowerTrainer):\n",
    "    def __init__(self, input, model):\n",
    "        #\"\"\"\n",
    "        #Args:\n",
    "        #    input (InputSource):\n",
    "        #    model (GANModelDesc):\n",
    "        #\"\"\"\n",
    "        super(GANTrainer, self).__init__()\n",
    "#         assert isinstance(model, GANModelDesc), model # GANModelDesc 에서 정의된 model을 사용하는 것이 맞는지 확인하기 위함.\n",
    "#         # 여기서 model은 GANModelDesc 에서 정의된 model이다. 근데 어떻게 가지고 온것일까? 아니면 나중에 Trainconfig()에서 training 시킬때 parameter로 GANTrainer(self, input, model)에서 model = GANModelDesc()하는 것일까? \n",
    "        assert isinstance(model, Model), model ### ****** checkpoint!!!\n",
    "        inputs_desc = model.get_inputs_desc() # GANModelDesc or Model 에서 정의된 model의 input list을 가져와 input_desc에 assgin한다.  \n",
    "        # Setup input\n",
    "        cbs = input.setup(inputs_desc) # callback 시 필요한 input 데이터 셋업\n",
    "        self.register_callback(cbs)\n",
    "        \n",
    "        #\"\"\"\n",
    "        #We need to set tower_func because it's a TowerTrainer,\n",
    "        #and only TowerTrainer supports automatic graph creation for inference during training.\n",
    "        #If we don't care about inference during training, using tower_func is\n",
    "        #not needed. Just calling model.build_graph directly is OK.\n",
    "        #\"\"\"\n",
    "        \n",
    "        # build the graph ### 얘는 GANModelDesc interface로 정의된 model을 \n",
    "        self.tower_func = TowerFuncWrapper(model.build_graph, inputs_desc) # GANModelDesc 에서 정의된 model의 input list를 사용하여 training할 tower(model) function으로 wrapping한다. \n",
    "        with TowerContext('', is_training=True):\n",
    "            self.tower_func(*input.get_input_tensors()) ## *input 대신에 뭘 넣어줘야하는거 같은데.. 아닌가? 확인해보자!\n",
    "            #tower_func(*input.get_input_tensors()) : ###\n",
    "            \n",
    "        opt = model.get_optimizer() # Model interface의 정의된 optimizer(self) 함수의 d_opt, g_opt를 반환한다.\n",
    "        # opt : self.optimizer()\n",
    "        \n",
    "        # define the training iteration(with using g_vars, d_vars)\n",
    "        # by default, run one d_min after one g_min\n",
    "        \n",
    "        with tf.name_scope('optimize'): # 여기서 opt은 그냥 self.optimizer() 이니까 AdamOptimizer()를 사용해야할거 같은데, d_train_step = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=dvar) 참고\n",
    "            g_min = opt.minimize(model.g_loss, var_list=model.g_vars, name='g_opt') # Model class에서 정의한 opt을 Adam으로 해주자.\n",
    "            with tf.control_dependencies([g_min]):\n",
    "                d_min = opt.minimize(model.d_loss, var_list=model.d_vars, name='d_opt')\n",
    "        \n",
    "        self.train_op = d_min\n",
    "\n",
    "        \n",
    "class SeparateGANTrainer(TowerTrainer):\n",
    "    #\"\"\" A GAN trainer which runs two optimization ops with a certain ratio.\"\"\"\n",
    "    def __init__(self, input, model, d_period=1, g_period=1):\n",
    "        #\"\"\"\n",
    "        #Args:\n",
    "        #    d_period(int): period of each d_opt run\n",
    "        #    g_period(int): period of each g_opt run\n",
    "        #\"\"\"\n",
    "        super(SeparateGANTrainer, self).__init__() # super : 상위 클래스의 initializing을 중복해서 하지마라, 지금 있는 클래스의 init만 하자.\n",
    "        self._d_period = int(d_period)\n",
    "        self._g_period = int(g_period)\n",
    "        assert min(d_period, g_period) == 1 # assert 다음에 나오는 값이 false면 error 발생, 이걸 확인해주기 위해 assert를 써준다. 어디서 에러가 발생했는지 알기 위해서, debug를 잘 하기 위해 사용하는 방법\n",
    "        \n",
    "        # setup input\n",
    "        cbs = input.setup(model.get_inputs_desc())\n",
    "        self.register_callback(cbs)\n",
    "        \n",
    "        # build the graph\n",
    "        self.tower_func = TowerFuncWrapper(model.build_graph, model.get_inputs_desc())\n",
    "        with TowerContext('', is_training=True):\n",
    "            self.tower_func(*input.get_input_tensors())\n",
    "            \n",
    "        opt = model.get_optimizer()\n",
    "        with tf.name_scope('optimize'):\n",
    "            self.d_min = opt.minimize(\n",
    "                model.d_loss, var_list=model.d_vars, name='d_min')\n",
    "            self.g_min = opt.minimize(\n",
    "                model.g_loss, var_list=model.g_vars, name='g_min')\n",
    "            \n",
    "    def run_step(self):\n",
    "        # define the training iteration\n",
    "        if self.global_step % (self._d_period) == 0:\n",
    "            self.hooked_sess.run(self.d_min)\n",
    "        if self.global_step % (self._g_period) == 0:\n",
    "            self.hooked_sess.run(self.g_min)\n",
    "            \n",
    "\n",
    "class MultiGPUGANTrainer(TowerTrainer):\n",
    "    #\"\"\"\n",
    "    #A replacement of GANTrainer (optimize d and g one by one) with multi-gpu support.\n",
    "    #\"\"\"\n",
    "    def __init__(self, num_gpu, input, model):\n",
    "        super(MultiGPUGANTrainer, self).__init__()\n",
    "        assert num_gpu > 1\n",
    "        raw_devices = ['/gpu;{}'.format(k) for k in range(num_gpu)]\n",
    "        \n",
    "        # setup input\n",
    "        input = StagingInput(input)\n",
    "        cbs = input.setup(model.get_inputs_desc())\n",
    "        self.register_callback(cbs)\n",
    "        \n",
    "        # build the graph with multi-gpu replication\n",
    "        def get_cost(*inputs):\n",
    "            model.build_graph(*inputs)\n",
    "            return [model.d_loss, model.g_loss]\n",
    "        \n",
    "        self.tower_func = TowerFuncWrapper(get_cost, model.get_inputs_desc())\n",
    "        devices = [LeastLoadedDeviceSetter(d, raw_devices) for d in raw_devices]\n",
    "        cost_list = DataParallelBuilder.build_on_towers(\n",
    "            list(range(num_gpu)),\n",
    "            lambda: self.tower_func(*input.get_input_tensors()),\n",
    "            devices)\n",
    "        # simply average the cost here. It might be faster to average the gardients\n",
    "        with tf.name_scope('optimize'):\n",
    "            d_loss = tf.add_n([x[0] for x in cost_list]) * (1.0 / num_gpu)\n",
    "            g_loss = tf.add_n([x[1] for x in cost_list]) * (1.0 / num_gpu)\n",
    "\n",
    "            opt = model.get_optimizer()\n",
    "            # run one d_min after one g_min\n",
    "            g_min = opt.minimize(g_loss, var_list=model.g_vars,\n",
    "                                    colocate_gradients_with_ops=True, name='g_opt')\n",
    "            with tf.control_dependencies([g_min]):\n",
    "                d_min = opt.minimize(d_loss, var_list=model.d_vars,\n",
    "                                        colocate_gradients_with_ops=True, name='d_opt')\n",
    "            \n",
    "            # define the training iteration\n",
    "            self.train_op = d_min\n",
    "            \n",
    "class RandomZData(DataFlow):\n",
    "       def __init__(self, shape):\n",
    "            super(RandomZData, self).__init__()\n",
    "            self.shape = shape\n",
    "       \n",
    "       def get_data(self):\n",
    "            while True:\n",
    "                yield [np.random.uniform(-1, 1, size=self.shape)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import cv2 # for using AugmentImageComponent\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from GAN import *\n",
    "from PIL import Image\n",
    "from tensorpack import *\n",
    "from tensorpack.dataflow import *\n",
    "from tensorpack.callbacks import *\n",
    "from tensorpack.tfutils import summary\n",
    "from tensorpack.utils.viz import stack_patches\n",
    "from tensorpack.tfutils.scope_utils import auto_reuse_variable_scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get DataFlow\n",
    "### load mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataflow(batch_size, is_train='train'): # str type\n",
    "    # bool_is_train = is_train == 'train' : 우리는 T/F로 코딩을 하는 경우가 많다. \n",
    "    df = dataset.Mnist(is_train, shuffle=True) # return type: dataflow(df)\n",
    "    # dataset에 있는 Mnist data를 가지고 온다. FashionMnist Data도 있다. \n",
    "    # ----- Image Augmentation Options -------- #\n",
    "    if is_train is 'train':\n",
    "        augs = [\n",
    "            #   imgaug.CenterCrop(256, 256),\n",
    "            #   imgaug.Resize((225, 225)),\n",
    "            #   imgaug.Grayscale(keepdims=True),\n",
    "#                imgaug.Flip(horiz=True, vert=False, prob=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        augs = [\n",
    "            #   imgaug.CenterCrop(256, 256),\n",
    "            #   imgaug.Resize((225, 225)),\n",
    "        ]\n",
    "    df = AugmentImageComponent(df, augs)\n",
    "    # group data into batches of size 128\n",
    "    df = BatchData(df, batch_size)\n",
    "    # start 3 processes to run the dataflow in parallel\n",
    "    # df = PrefetchDataZMQ(df, 10, multiprocessing.cpu_count())\n",
    "    # df = PrefetchDataZMQ(df, 64) # : batch size = 64\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[0823 23:52:53 @fs.py:100]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Env var $TENSORPACK_DATASET not set, using /root/tensorpack_data for datasets.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2016x2016 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = get_dataflow(4, 'train') #(batch_size, )\n",
    "df.reset_state() # 보통 안해줘도 된다. \n",
    "\n",
    "fig =plt.figure(figsize=(28, 28))\n",
    "\n",
    "for idx, dp in enumerate(df.get_data()):\n",
    "    if idx == 0:\n",
    "        for i in range(4):\n",
    "            img = dp[idx][i]\n",
    "            fig.add_subplot(1, 4, i + 1)\n",
    "            plt.imshow(img, cmap='gray')\n",
    "        plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model \n",
    "#### Model includes Loss function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(GANModelDesc):\n",
    "#     # hyperparameter 설정\n",
    "#         num_epoch = 100000 # 에폭 수가 크면 로컬 보다는 GPU로, Training 단계에서 설정, main.py -  \n",
    "#         batch_size = 64 # Training 단계에서 설정, main.py - \n",
    "    def __init__(self, num_input, num_latent_variable, num_hidden, batch, learning_rate):\n",
    "        self.NumInput = num_input\n",
    "        self.NumLatentVariable = num_latent_variable\n",
    "        self.NumHidden = num_hidden\n",
    "        self.batch = batch\n",
    "        self.LearningRate = learning_rate\n",
    "        \n",
    "#     global num_input \n",
    "#     global num_latent_variable \n",
    "#     global num_hidden \n",
    "#     global learning_rate\n",
    "        \n",
    "    def inputs(self):\n",
    "        \"\"\"\n",
    "        Define input shape, MNIST data\n",
    "        \"\"\"\n",
    "        return [tf.placeholder(tf.float32, [None, self.NumInput], 'X')] # input image size\n",
    "                 \n",
    "        \n",
    "    # Generator 생성하는 함수 정의 \n",
    "    # Inputs: \n",
    "    #     Z : input Latent Variable (noise variable)\n",
    "    # Output:\n",
    "    #    generated_mnist_image : Generator로부터  생성된 MNIST 이미지\n",
    "    def build_generator(self, Z): # inputs() 에서 반환될 리스트 중 2번째 요소 Z가 들어와야함\n",
    "        # Generator 함수에서 사용되는 변수들 설정\n",
    "        # 100 -> 128 -> 784 : layer 통과 할 때마다 image 개수 num_input 수에 맞추기.\n",
    "        with tf.variable_scope('generator'):\n",
    "            # hidden layer paremeter\n",
    "            G_W1 = tf.Variable(tf.random_normal(shape=[self.NumLatentVariable, self.NumHidden], stddev=5e-2))\n",
    "            G_b1 = tf.Variable(tf.constant(0.1, shape=[self.NumHidden]))\n",
    "            # output layer parameter\n",
    "            G_W2 = tf.Variable(tf.random_normal(shape=[self.NumHidden, self.NumInput], stddev=5e-2))\n",
    "            G_b2 = tf.Variable(tf.constant(0.1, shape=[self.NumInput]))\n",
    "            \n",
    "            hidden_layer = tf.nn.relu((tf.matmul(Z, G_W1) + G_b1))\n",
    "            output_layer = tf.matmul(hidden_layer, G_W2) + G_b2\n",
    "            generated_mnist_image = tf.nn.sigmoid(output_layer)\n",
    "    \n",
    "        return generated_mnist_image # 생성된 이미지 반환.\n",
    "\n",
    "    # Discriminator를 생성하는 함수를 정의\n",
    "    # Inputs:\n",
    "    #   X : 인풋 이미지\n",
    "    # Output:\n",
    "    #   predicted_value : Discriminator가 판단한 True(1) or Fake(0)\n",
    "    #   logits : sigmoid를 씌우기전의 출력값\n",
    "    def build_discriminator(self, X):\n",
    "        # Discriminator 함수에서 사용되는 변수들 설정 \n",
    "        # 784 -> 128 -> 1 : Generator에서 생성된 784 image 들을 1개로 만들어 인풋 이미지와 구별\n",
    "        with tf.variable_scope('discriminator'):\n",
    "            # hidden layer parameter\n",
    "            D_W1 = tf.Variable(tf.random_normal(shape=[self.NumInput, self.NumHidden], stddev=5e-2))\n",
    "            D_b1 = tf.Variable(tf.constant(0.1, shape=[self.NumHidden]))\n",
    "            # output layer parameter\n",
    "            D_W2 = tf.Variable(tf.random_normal(shape=[self.NumHidden, 1], stddev=5e-2))\n",
    "            D_b2 = tf.Variable(tf.constant(0.1, shape=[1]))\n",
    "            hidden_layer = tf.nn.relu((tf.matmul(X, D_W1) + D_b1))\n",
    "            logits = tf.matmul(hidden_layer, D_W2) + D_b2\n",
    "            predicted_value = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def build_graph(self, X): # mapping input data automatically\n",
    "        # build a cost_func (loss function) -> build_losses(), need logits_real, fake.\n",
    "        # collect_variables(self, g_scope='gen', d_scope='discrim')\n",
    "        # build_losses(self, logits_real, logits_fake)\n",
    "        \n",
    "        #Z = tf.random_uniform([self.batch, self.NumLatentVariable], -1, 1,  'Z_train')\n",
    "        Z = tf.random_uniform([self.batch, self.NumLatentVariable], -1, 1, name='Z_train')\n",
    "        Z = tf.placeholder_with_default(Z, [None, self.NumLatentVariable], name='Z')\n",
    "            \n",
    "            \n",
    "\n",
    "        with tf.variable_scope('Gen'):\n",
    "            gen_image = self.build_generator(Z)\n",
    "        tf.summary.image('Generated_image', gen_image, max_outputs=25)\n",
    "            \n",
    "        with tf.variable_scope('Discrim'):\n",
    "            \n",
    "            logits_real = self.build_discriminator(X)\n",
    "            logits_fake = self.build_discriminator(gen_image)   \n",
    "        \n",
    "        self.build_losses(logits_real, logits_fake)\n",
    "        self.collect_variables() # def collect_variables(self, g_scope='gen', d_scope='discrim'):\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    def optimizer(self): # trainer 에서 opt\n",
    "        lr = self.LearningRate\n",
    "        return tf.train.AdamOptimizer(lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. main.py\n",
    "\n",
    "### Main - Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for tensorboard \n",
    "# logger.set_logger_dir('./GAN_mnist_result')\n",
    "# # logger.auto_set_dir() : command 창에서 사용, jupyter 에서는 사용불가.\n",
    "\n",
    "# df = get_dataflow(100, 'train') # get_dataflow(batch_size, is_train='train')\n",
    "# df_test = get_dataflow(100, 'test')\n",
    "\n",
    "# df.reset_state()\n",
    "# df_test.reset_state()\n",
    "\n",
    "# def get_dataflow(batch_size, is_train='train'):\n",
    "df = get_dataflow(100, 'train') # batch size : 100  \n",
    "steps_per_epoch = df.size() # 60000/100, df = BatchData(df, batch_size)\n",
    "\n",
    "# In Model class \n",
    "# def __init__(self, num_input, num_latent_variable, num_hidden, batch, learning_rate):\n",
    "num_input = 28*28\n",
    "num_latent_variable = 100 \n",
    "num_hidden = 128 \n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = Model(num_input, num_latent_variable, num_hidden, batch_size, learning_rate) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[0824 00:17:44 @logger.py:109]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Log directory ./GAN_mnist_log exists! Use 'd' to delete it. \n",
      "\u001b[32m[0824 00:17:44 @logger.py:112]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m If you're resuming from a previous run, you can choose to keep it.\n",
      "Press any other key to exit. \n",
      "Select Action: k (keep) / d (delete) / q (quit):d\n",
      "\u001b[32m[0824 00:17:46 @logger.py:74]\u001b[0m Argv: /opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py -f /root/.local/share/jupyter/runtime/kernel-c5ac2c97-d36f-4d6c-b99a-22ab56f7d63f.json\n",
      "\u001b[32m[0824 00:17:46 @input_source.py:202]\u001b[0m Setting up the queue 'QueueInput_2/input_queue' for CPU prefetching ...\n",
      "\u001b[32m[0824 00:17:46 @training.py:113]\u001b[0m Building graph for training tower 0 ...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5d7372d327c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     MultiGPUGANTrainer(num_gpu = 4,\n\u001b[1;32m     20\u001b[0m                       \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQueueInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_with_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                                         callbacks=[PeriodicTrigger(\n\u001b[1;32m     23\u001b[0m                                                 \u001b[0mModelSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./GAN_mnist_result'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/modulabs/peter/wooree/GAN.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_gpu, input, model)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtower_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             devices)\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Simply average the cost here. It might be faster to average the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'optimize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorpack/graph_builder/training.py\u001b[0m in \u001b[0;36mbuild_on_towers\u001b[0;34m(towers, func, devices, use_vs)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# so these duplicated variables won't be saved by default.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0moverride_to_local_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musevs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                     \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/modulabs/peter/wooree/GAN.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m         cost_list = DataParallelBuilder.build_on_towers(\n\u001b[1;32m    170\u001b[0m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtower_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             devices)\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Simply average the cost here. It might be faster to average the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorpack/tfutils/tower.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_current_tower_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Function must be called under TowerContext!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tower_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTowerTensorHandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inputs_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/modulabs/peter/wooree/GAN.py\u001b[0m in \u001b[0;36mget_cost\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# Build the graph with multi-gpu replication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-118d76be8821>\u001b[0m in \u001b[0;36mbuild_graph\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# def collect_variables(self, g_scope='gen', d_scope='discrim'):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/modulabs/peter/wooree/GAN.py\u001b[0m in \u001b[0;36mcollect_variables\u001b[0;34m(self, g_scope, d_scope)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \"\"\"\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAINABLE_VARIABLES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAINABLE_VARIABLES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuration containing everything necessary in a training.\n",
    "\"\"\"\n",
    "config = TrainConfig(\n",
    "    model = model,\n",
    "    data = QueueInput(df), # 상황에 따라서 ZMQ사용도 가능, \n",
    "    callbacks = [\n",
    "        # save the model after every epoch\n",
    "        ModelSaver(),\n",
    "        \n",
    "    ],\n",
    "    steps_per_epoch = steps_per_epoch, # steps_per_epoch = df.size()\n",
    "    max_epoch = 100000,\n",
    ")\n",
    "\n",
    "# training with CPU or GPU?\n",
    "if tf.test.gpu_device_name():\n",
    "    logger.set_logger_dir('./GAN_mnist_log')\n",
    "    MultiGPUGANTrainer(num_gpu = 4,\n",
    "                      input = QueueInput(df),\n",
    "                      model = model).train_with_defaults(\n",
    "                                        callbacks=[PeriodicTrigger(\n",
    "                                                ModelSaver(checkpoint_dir='./GAN_mnist_result'),\n",
    "                                                every_k_epochs=10)],\n",
    "                                        steps_per_epoch = steps_per_epoch,\n",
    "                                        max_epoch = 100)\n",
    "else:\n",
    "    GANTrainer(input = QueueInput(df),\n",
    "                      model = model).train_with_defaults(\n",
    "                                        callbacks=[PeriodicTrigger(\n",
    "                                                ModelSaver(checkpoint_dir='./GAN_mnist_result'),\n",
    "                                                every_k_epochs=10)],\n",
    "                                        steps_per_epoch = steps_per_epoch,\n",
    "                                        max_epoch = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the original Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = get_dataflow(25, 'train')\n",
    "origin.reset_state()\n",
    "\n",
    "for idx, dp in enumerate(origin.get_data()):\n",
    "    if idx == 0:\n",
    "        dp = dp[0] + 1\n",
    "        dp = dp * 90.0\n",
    "        img = stack_patches(dp, nr_row=5, nr_col=5)  \n",
    "        \n",
    "        temp = img.reshape(332, 332)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(temp, cmap='gray')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_input, num_latent_variable, num_hidden, batch_size, learning_rate)\n",
    "\n",
    "# loading the saved model.\n",
    "session = get_model_loader('./GAN_mnist_log/model-60000.index')\n",
    "\n",
    "pred_config = PredictConfig(\n",
    "        model = model,\n",
    "        session_init = session,\n",
    "        input_names = ['Z'],\n",
    "        output_names = ['gen/gen', 'Z']\n",
    "    )\n",
    "\n",
    "# make 25 images\n",
    "pred = SimpleDatasetPredictor(pred_config, RandomZData((25, 100)))\n",
    "\n",
    "for i, gen in enumerate(pred.get_result()):  \n",
    "    gen = gen[0] + 1\n",
    "    gen = gen * 90.0\n",
    "    gen = np.clip(gen, 0, 255)\n",
    "    gen = gen[:, :, :, ::-1]\n",
    "    img = stack_patches(gen, nr_row=5, nr_col=5)\n",
    "    \n",
    "    temp = img.reshape(332, 332)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(temp, cmap='gray')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_input, num_latent_variable, num_hidden, batch_size, learning_rate)\n",
    "session = get_model_loader('./GAN_mnist_result/model-6000.index')\n",
    "\n",
    "pred_config = PredictConfig(\n",
    "        model=model,\n",
    "        session_init=session,\n",
    "        input_names=['X', 'Z'], # class Model-inputs\n",
    "        output_names=['accuracy', 'Loss'] # class Model-build_graph\n",
    "    )\n",
    "pred = SimpleDatasetPredictor(pred_config, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0.0\n",
    "total_loss = 0.0\n",
    "\n",
    "for i, (acc, loss) in enumerate(pred.get_result()): # 각 batch마다 pred에서 get_result\n",
    "    accuracy += acc \n",
    "    total_loss += loss\n",
    "    \n",
    "print(\"Accuracy: {:0.3f}\".format(accuracy/(i+1)))\n",
    "print(\"Loss: {:0.3f}\".format(total_loss/(i+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
