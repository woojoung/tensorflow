{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 텐서플로우(TensorFlow)를 이용한 MNIST 데이터 생성\n",
    "\n",
    "TensorFlow를 이용해서 MNIST 데이터의 분포를 학습하는 GAN 모델을 구현해보자.\n",
    "\n",
    "참고\n",
    "솔라리스의 인공지능 연구실\n",
    "http://solarisailab.com/archives/2482"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaeujeong/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/chaeujeong/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/chaeujeong/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/chaeujeong/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/chaeujeong/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/chaeujeong/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/chaeujeong/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-d23190aa25dd>:16: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/chaeujeong/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/chaeujeong/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist.data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/chaeujeong/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist.data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/chaeujeong/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./mnist.data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist.data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/chaeujeong/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "반복(Epoch): 0, Generator 손실함수(g_loss): 1.068597, Discriminator 손실함수(d_loss): 1.520776\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "GAN(Generative Adversarial Networks)을 이용한 MNIST 데이터 생성\n",
    "Reference : https://github.com/TengdaHan/GAN-TensorFlow\n",
    "Author : woojoung\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "\n",
    "# MNIST 데이터 로딩\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist.data\", one_hot=True)\n",
    "\n",
    "# Generator에서 생성된 MNIST 이미지를 8x8 grid로 보여주기 위해 plot 함수 정의\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    gs = gridspec.GridSpec(8, 8)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i]) # gs[i] : 8x8 grid로 subplot 그려주기 \n",
    "        plt.axis('off')\n",
    "        plt.imshow(sample.reshape(28, 28))\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# hyperparameter 설정\n",
    "num_epoch = 100000 # 에폭 크면 로컬 보다는 GPU로 돌리자. 예제에선 100000\n",
    "batch_size = 64 # 예제에선 64\n",
    "num_input = 28*28\n",
    "num_latent_variable = 100 \n",
    "num_hidden = 128 \n",
    "learning_rate = 0.001\n",
    "\n",
    "# placeholder 선언\n",
    "X = tf.placeholder(tf.float32, [None, num_input])           # : input image\n",
    "z = tf.placeholder(tf.float32, [None, num_latent_variable]) # : imput Latent variable\n",
    "\n",
    "# Generator 함수에서 사용되는 변수들 설정\n",
    "# 100 -> 128 -> 784 : layer 통과 할 때마다 image 개수 num_input 수에 맞추기.\n",
    "with tf. variable_scope('generator'):\n",
    "    # hidden layer paremeter\n",
    "    G_W1 = tf.Variable(tf.random_normal(shape=[num_latent_variable, num_hidden], stddev=5e-2))\n",
    "    G_b1 = tf.Variable(tf.constant(0.1, shape=[num_hidden]))\n",
    "    # output layer parameter\n",
    "    G_W2 = tf.Variable(tf.random_normal(shape=[num_hidden, num_input], stddev=5e-2))\n",
    "    G_b2 = tf.Variable(tf.constant(0.1, shape=[num_input]))\n",
    "    \n",
    "# Discriminator 함수에서 사용되는 변수들 설정 \n",
    "# 784 -> 128 -> 1 : Generator에서 생성된 784 image 들을 1개로 만들어 인풋 이미지와 구별\n",
    "with tf.variable_scope('discriminator'):\n",
    "    # hidden layer parameter\n",
    "    D_W1 = tf.Variable(tf.random_normal(shape=[num_input, num_hidden], stddev=5e-2))\n",
    "    D_b1 = tf.Variable(tf.constant(0.1, shape=[num_hidden]))\n",
    "    # output layer parameter\n",
    "    D_W2 = tf.Variable(tf.random_normal(shape=[num_hidden, 1], stddev=5e-2))\n",
    "    D_b2 = tf.Variable(tf.constant(0.1, shape=[1]))\n",
    "    \n",
    "# Generator 생성하는 함수 정의 \n",
    "# Inputs: \n",
    "#     X : input Latent Variable\n",
    "# Output:\n",
    "#    generated_mnist_image : Generator로부터  생성된 MNIST 이미지\n",
    "def build_generator(X):\n",
    "    hidden_layer = tf.nn.relu((tf.matmul(X, G_W1) + G_b1))\n",
    "    output_layer = tf.matmul(hidden_layer, G_W2) + G_b2\n",
    "    generated_mnist_image = tf.nn.sigmoid(output_layer) # 왜 output image를 생성하는 layer에서 활성함수를 relu 대신에 sigmoid를 사용하였을까? \n",
    "    \n",
    "    return generated_mnist_image # 생성된 이미지 반환.\n",
    "\n",
    "# Discriminator를 생성하는 함수를 정의\n",
    "# Inputs:\n",
    "#   X : 인풋 이미지\n",
    "# Output:\n",
    "#   predicted_value : Discriminator가 판단한 True(1) or Fake(0)\n",
    "#   logits : sigmoid를 씌우기전의 출력값\n",
    "def build_discriminator(X):\n",
    "    hidden_layer = tf.nn.relu((tf.matmul(X, D_W1) + D_b1))\n",
    "    logits = tf.matmul(hidden_layer, D_W2) + D_b2 # logits : 입력 데이터를 네트워크를 통해 전방향 진행하여 나온 결과물을 logits 이라고 부른다.\n",
    "    predicted_value = tf.nn.sigmoid(logits)     \n",
    "    \n",
    "    return predicted_value, logits # 왜 2개 반환? \n",
    "\n",
    "# Generator 선언\n",
    "G = build_generator(z) # : imput Latent variable\n",
    "\n",
    "# Discriminator 선언\n",
    "D_real, D_real_logits = build_discriminator(X) # D(X), X: input image, size 28x28, 784\n",
    "D_fake, D_fake_logits = build_discriminator(G) # D(G(z)), G: generated_mnist_image, size num_imput, 784\n",
    "\n",
    "# loss function of Discriminator, Discriminator의 손실 함수 정의.\n",
    "d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real_logits)))    # log(D(x)). # labels: A Tensor of the same type and shape as logits.\n",
    "#### real이라고 구별할 때 loss\n",
    "d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros_like(D_fake_logits)))   # log(1-D(G(z)))\n",
    "#### fake라고 구별할 때 loss\n",
    "d_loss = d_loss_real + d_loss_fake  # log(D(x)) + log(1-D(G(z)))\n",
    "#### loss의 합, D의 최종적인 loss\n",
    "\n",
    "##### labels : logits과 똑같은 type과 shape인데 ones, zeros에 따라 값이 1이냐 0. real은 1, fake는 0\n",
    "# tf.ones_like(tensor): this operation returns a tensor of the same type and shape as tensor with all elements set to 1.\n",
    "# tf.zeros_like(tensor): this operation returns a tensor of the same type and shape as tensor with all elements set to zero.\n",
    "#####\n",
    "\n",
    "# loss function of Generator, Generator의 손실 함수 정의.\n",
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones_like(D_fake_logits)))  #labels=tf.ones_like 와 labels=tf.zeros_like의 차이는? \n",
    "#### generator의 labels은 Discriminator에서 fake지만 1(real)이라고 인식 시켜야 하므로 labels=tf.ones_like(D_fake_logits).  \n",
    "\n",
    "# 전체 파라미터를 Discriminator와 관련된 파라미터와 Generator와 관련된 파라미터로 나눕니다.\n",
    "tvar = tf.trainable_variables() # tensorflow graph단에 new variable을 add.\n",
    "dvar = [var for var in tvar if 'discriminator' in var.name]\n",
    "gvar = [var for var in tvar if 'generator' in var.name]\n",
    "#####\n",
    "# tf.trainable_variables() : returns a list of Variable objects.\n",
    "# Returns all variables created with trainable=True.\n",
    "# When passed trainable=True, the Variable() constructor automatically adds new variables \n",
    "# to the graph collection GraphKeys.TRAINABLE_VARIABLES. \n",
    "# This convenience function returns the contents of that collection.\n",
    "#####\n",
    "\n",
    "# Discriminator와 Generator의 Optimizer를 정의\n",
    "d_train_step = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=dvar)\n",
    "g_train_step = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=gvar)\n",
    "\n",
    "# 생성된 이미지들을 저장할 generated_outputs 폴더를 생성합니다. \n",
    "num_img = 0\n",
    "if not os.path.exists('generated_output/'):\n",
    "    os.makedirs('generated_output/')\n",
    "\n",
    "#####\n",
    "# os.path.exists(path)\n",
    "# Return True if path refers to an existing path or an open file descriptor. \n",
    "# Returns False for broken symbolic links. \n",
    "# On some platforms, this function may return False if permission is not granted to execute os.stat() on the requested file, even if the path physically exists.\n",
    "#####\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 변수들에 초기값을 할당한다. \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # num_epoch 횟수만큼 최적화를 수행한다. \n",
    "    for i in range(num_epoch):\n",
    "        # MNIST 이미지를 batch_size 만큼 불러온다.\n",
    "        batch_X, _ = mnist.train.next_batch(batch_size)\n",
    "        # Latent Variable의 인풋으로 사용할 noise를 Uniform Distribution에서 batch_size만큼 샘플링한다.\n",
    "        batch_noise = np.random.uniform(-1., 1., [batch_size, 100]) # batch size만큼 랜덤으로 샘플링한다.\n",
    "        \n",
    "        # 500번 반복할때마다 생성된 이미지를 저장한다.\n",
    "        if i % 500 == 0:\n",
    "            samples = sess.run(G, feed_dict={z: np.random.uniform(-1., 1., [64, 100])}) # z: latent variable의 placeholder\n",
    "            fig = plot(samples)\n",
    "            plt.savefig('generated_output/%s.png' % str(num_img).zfill(3), bbox_inches='tight')\n",
    "            num_img += 1\n",
    "            plt.close(fig)\n",
    "        \n",
    "        # Discriminator 최적화를 수행하고 Discriminator의 손실함수를 return한다. \n",
    "        _, d_loss_print = sess.run([d_train_step, d_loss], feed_dict={X: batch_X, z: batch_noise}) # 원본 이미지 와 노이즈 이미지 두 개를 비교.\n",
    "        \n",
    "        # Generator 최적화를 수행하고 Generator 손실함수를 return한다.\n",
    "        _, g_loss_print = sess.run([g_train_step, g_loss], feed_dict={z: batch_noise})\n",
    "            \n",
    "        # 100번 반복할때마다 Discriminator의 손실함수와 Generator의 손실함수를 출력한다. \n",
    "        if i % 100 == 0:\n",
    "            print('(Epoch): %d, Generator loss(g_loss): %f, Discriminator loss(d_loss): %f' % (i, g_loss_print, d_loss_print))\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반복(Epoch): 0, Generator 손실함수(g_loss): 1.668353, Discriminator 손실함수(d_loss): 1.276711\n",
      "반복(Epoch): 1, Generator 손실함수(g_loss): 2.129136, Discriminator 손실함수(d_loss): 1.015499\n",
      "반복(Epoch): 2, Generator 손실함수(g_loss): 2.469712, Discriminator 손실함수(d_loss): 0.855583\n",
      "반복(Epoch): 3, Generator 손실함수(g_loss): 2.710187, Discriminator 손실함수(d_loss): 0.719882\n",
      "반복(Epoch): 4, Generator 손실함수(g_loss): 2.861951, Discriminator 손실함수(d_loss): 0.582250\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # 변수들에 초기값을 할당한다. \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # num_epoch 횟수만큼 최적화를 수행한다. \n",
    "    for i in range(num_epoch):\n",
    "        # MNIST 이미지를 batch_size 만큼 불러온다.\n",
    "        batch_X, _ = mnist.train.next_batch(batch_size)\n",
    "        # Latent Variable의 인풋으로 사용할 noise를 Uniform Distribution에서 batch_size만큼 샘플링한다.\n",
    "        batch_noise = np.random.uniform(-1., 1., [batch_size, 100])\n",
    "        \n",
    "        # 500번 반복할때마다 생성된 이미지를 저장한다.\n",
    "        \n",
    "        samples = sess.run(G, feed_dict={z: np.random.uniform(-1., 1., [64, 100])}) # z: latent variable의 placeholder\n",
    "        fig = plot(samples)\n",
    "        plt.savefig('generated_output/%s.png' % str(num_img).zfill(3), bbox_inches='tight')\n",
    "        num_img += 1\n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Discriminator 최적화를 수행하고 Discriminator의 손실함수를 return한다. \n",
    "        _, d_loss_print = sess.run([d_train_step, d_loss], feed_dict={X: batch_X, z: batch_noise})\n",
    "        \n",
    "        # Generator 최적화를 수행하고 Generator 손실함수를 return한다.\n",
    "        _, g_loss_print = sess.run([g_train_step, g_loss], feed_dict={z: batch_noise})\n",
    "            \n",
    "        # 100번 반복할때마다 Discriminator의 손실함수와 Generator의 손실함수를 출력한다. \n",
    "        \n",
    "        print('반복(Epoch): %d, Generator 손실함수(g_loss): %f, Discriminator 손실함수(d_loss): %f' % (i, g_loss_print, d_loss_print))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ones_like:0' shape=(2, 3) dtype=int32>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tensor = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "tf.ones_like(tensor)  # [[1, 1, 1], [1, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ones_like_1:0\", shape=(2, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.ones_like(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const:0' shape=(2, 3) dtype=int32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(2, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(tf.ones_like(tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tvar = tf.trainable_variables()\n",
    "tvar\n",
    "print(sess.run(tvar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dvar = [var for var in tvar if 'discriminator' in var.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
